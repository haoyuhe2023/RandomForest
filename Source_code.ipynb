{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ“ Decision tree (without stop-split-early condition)\n",
    "It includes the following components:\n",
    "1) Calculate information gain; (10 marks)\n",
    "2) split the data via finding the best feature based on a); (10 marks)\n",
    "3) create branches recursively based on b) until every leaf only contains a single category; (10 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of DecisionTree: 0.8210105052526263\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Define a function to randomly choose 10000 rows form 'data.csv'\n",
    "def select_rows(input_file, output_file):\n",
    "    with open(input_file, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        rows = [row for row in reader]\n",
    "    first_row = [rows[0]]\n",
    "    random_rows = random.sample(rows[1:], 9999)\n",
    "    result = first_row + random_rows\n",
    "    with open(output_file, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(result)\n",
    "# define a node class for the Decision Tree\n",
    "class Node:\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.left = None   # left child of the node\n",
    "        self.right = None   # right child of the node\n",
    "        self.feature_idx = None # index of the features for splitting\n",
    "        self.threshold = None #threshold for splitting\n",
    "        self.label = None # predicted label for the node\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2):\n",
    "        self.min_samples_split = min_samples_split # Define minimum number of samples required to split a node\n",
    "        self.root = None # root of the tree\n",
    "\n",
    "    def calculate_entropy(self, labels):\n",
    "        # calculate entropy of a list of labels\n",
    "        _, counts = np.unique(labels, return_counts=True)\n",
    "        probabilities = counts / counts.sum()\n",
    "        entropy = sum(probabilities * -np.log2(probabilities))\n",
    "        return entropy\n",
    "\n",
    "    def calculate_info_gain(self, parent, left_child, right_child):\n",
    "        # Calculate the information gain of a split\n",
    "        weighted_child_entropy = sum(\n",
    "            [len(child_labels) / len(parent) * self.calculate_entropy(child_labels) for child_labels in\n",
    "             [left_child, right_child]])\n",
    "        parent_entropy = self.calculate_entropy(parent)\n",
    "        info_gain = parent_entropy - weighted_child_entropy\n",
    "        return info_gain\n",
    "\n",
    "    def find_best_split(self, data, labels):\n",
    "        # find the best split of a node\n",
    "        n_features = len(data[0])\n",
    "        best_gain = -1\n",
    "        best_feature_idx = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            # generate possible split thresholds for the feature\n",
    "            thresholds = sorted(list(set([row[feature_idx] for row in data])))\n",
    "            for threshold in thresholds:\n",
    "                # partition the data and labels based on the split threshold\n",
    "                left_idxs = [i for i, row in enumerate(data) if row[feature_idx] < threshold]\n",
    "                right_idxs = [i for i, row in enumerate(data) if row[feature_idx] >= threshold]\n",
    "\n",
    "                if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "                    continue\n",
    "                gain = self.calculate_info_gain(labels, [labels[i] for i in left_idxs], [labels[i] for i in right_idxs])\n",
    "                # update the best information\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature_idx = feature_idx\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature_idx, best_threshold\n",
    "\n",
    "    def build_tree(self, data, labels):\n",
    "        n_samples = len(data)\n",
    "\n",
    "        # Base: only 1 sample or all samples belong to the same class\n",
    "        if len(set(labels)) == 1 or n_samples < self.min_samples_split:\n",
    "            return Node(data, labels)\n",
    "\n",
    "        # Recursive: find the best split and create sub-trees\n",
    "        best_feature_idx, best_threshold = self.find_best_split(data, labels)\n",
    "        left_idxs = [idx for idx in range(n_samples) if data[idx][best_feature_idx] <= best_threshold]\n",
    "        right_idxs = [idx for idx in range(n_samples) if data[idx][best_feature_idx] > best_threshold]\n",
    "\n",
    "        # Ensure both left and right child contain at least one sample\n",
    "        if len(left_idxs) > 0 and len(right_idxs) > 0:\n",
    "            left_data = [data[idx] for idx in left_idxs]\n",
    "            left_labels = [labels[idx] for idx in left_idxs]\n",
    "            right_data = [data[idx] for idx in right_idxs]\n",
    "            right_labels = [labels[idx] for idx in right_idxs]\n",
    "\n",
    "            node = Node(data, labels)\n",
    "            node.feature_idx = best_feature_idx\n",
    "            node.threshold = best_threshold\n",
    "            node.left = self.build_tree(left_data, left_labels)\n",
    "            node.right = self.build_tree(right_data, right_labels)\n",
    "\n",
    "            return node\n",
    "\n",
    "    def traverse_tree(self, x, node):\n",
    "        if node.label is not None:\n",
    "            return node.label\n",
    "        if x[node.feature_idx] <= node.threshold:\n",
    "            return self.traverse_tree\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self.build_tree(X, y)\n",
    "\n",
    "    def predict(self, data):\n",
    "        predictions = [] #initialize an empty list to store the predictions\n",
    "         # iterate over each sample in the input data startting from root node.\n",
    "        for sample in data:\n",
    "            node = self.root\n",
    "            while node.left and node.right:\n",
    "                # if the feature value for the current node is less than or equal to the threshold, follow the left child\n",
    "                if sample[node.feature_idx] <= node.threshold:\n",
    "                    node = node.left\n",
    "                # Otherwise follow right child\n",
    "                else:\n",
    "                    node = node.right\n",
    "            #append the most common label for the node to the predictions list\n",
    "            predictions.append(max(set(node.labels), key=node.labels.count))\n",
    "        return predictions\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    X = []  # features\n",
    "    y = []  # target labels\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # skip header row\n",
    "        for row in reader:\n",
    "            X.append([float(x) for x in row[:-1]])\n",
    "            y.append(int(row[-1]))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    # check the length to see if there is mistake\n",
    "    assert len(predictions) == len(labels)\n",
    "    #returns 1 if the prediction matches the actual label, and 0 otherwise.\n",
    "    n_correct = sum([1 if p == l else 0 for p, l in zip(predictions, labels)])\n",
    "    accuracy = n_correct / len(predictions)\n",
    "    return accuracy\n",
    "\n",
    "def k_fold_cross_validation(data, labels, k, model):\n",
    "    fold_size = len(data) // k\n",
    "    indices = list(range(len(data)))\n",
    "    # shuffle the data randomly and divide them into k groups\n",
    "    random.shuffle(indices)\n",
    "    accuracies = []\n",
    "    for i in range(k):\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size\n",
    "        # split all data into training and validation set\n",
    "        validation_indices = indices[start:end]\n",
    "        training_indices = indices[:start] + indices[end:]\n",
    "        training_data = [data[idx] for idx in training_indices]\n",
    "        training_labels = [labels[idx] for idx in training_indices]\n",
    "        validation_data = [data[idx] for idx in validation_indices]\n",
    "        validation_labels = [labels[idx] for idx in validation_indices]\n",
    "        model.fit(training_data, training_labels) # train the model on the training data\n",
    "        predictions = model.predict(validation_data)\n",
    "        accuracy = compute_accuracy(predictions, validation_labels)\n",
    "        accuracies.append(accuracy)\n",
    "    mean_accuracy = sum(accuracies) / k # sum all the accuracy and devide by K\n",
    "    return mean_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "select_rows('D:\\dts205tc\\data.csv', 'data1.csv')\n",
    "data, labels = load_data('data1.csv')\n",
    "tree = DecisionTree()\n",
    "mean_accuracy = k_fold_cross_validation(data, labels, k=5, model=tree)\n",
    "print(f\"Mean accuracy of DecisionTree: {mean_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset was too large for our devices to run the codes, more than 30 minutes were required. So we extract 10000 rows randomly from the original dataset to optimize the running time.The rest of the tasks will also perform on this extracted dataset 'data1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ“ Random Forest\n",
    "4) Bagging. Perform 100 Bootstrapping on the data, generate different decision trees based on task3), and perform majority-voting on their prediction results. (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of Bagging Tree: 0.8497248624312157\n",
      "Overall Accuracy: 0.8497849784978497\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from DecisionTree import DecisionTree, load_data,compute_accuracy;\n",
    "\n",
    "# Load data\n",
    "X, y = load_data('D:\\dts205tc\\data1.csv') \n",
    "\n",
    "# Initialize a list to store the trees\n",
    "trees = []\n",
    "\n",
    "# Perform 100 bootstrapping\n",
    "for i in range(100):\n",
    "    # Create a bootstrap sample\n",
    "    sample_indices = [random.randint(0, len(X) - 1) for _ in range(len(X))]\n",
    "    X_sample = [X[j] for j in sample_indices]\n",
    "    y_sample = [y[j] for j in sample_indices]\n",
    "\n",
    "    # Initialize and fit a decision tree\n",
    "    tree = DecisionTree()\n",
    "    tree.fit(X_sample, y_sample)\n",
    "\n",
    "    # Append the tree to the list\n",
    "    trees.append(tree)\n",
    "\n",
    "\n",
    "def k_fold_CV(data, labels, k, models):\n",
    "    fold_size = len(data) // k\n",
    "    indices = list(range(len(data)))\n",
    "    random.shuffle(indices)\n",
    "    accuracies = []\n",
    "    for i in range(k):\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size\n",
    "        validation_indices = indices[start:end]\n",
    "        training_indices = indices[:start] + indices[end:]\n",
    "        training_data = [data[idx] for idx in training_indices]\n",
    "        training_labels = [labels[idx] for idx in training_indices]\n",
    "        validation_data = [data[idx] for idx in validation_indices]\n",
    "        validation_labels = [labels[idx] for idx in validation_indices]\n",
    "        predictions = []\n",
    "        for sample in validation_data:\n",
    "            tree_predictions = [tree.predict([sample])[0] for tree in models]\n",
    "            prediction = max(set(tree_predictions), key=tree_predictions.count)\n",
    "            predictions.append(prediction)\n",
    "        accuracy = compute_accuracy(predictions, validation_labels)\n",
    "        accuracies.append(accuracy)\n",
    "    mean_accuracy = sum(accuracies) / k\n",
    "    return mean_accuracy\n",
    "\n",
    "\n",
    "# Set the number of folds\n",
    "k = 5\n",
    "\n",
    "# Calculate the mean accuracy over k folds\n",
    "mean_accuracy = k_fold_CV(X, y, k, trees)\n",
    "\n",
    "# Print the mean accuracy\n",
    "print(\"Mean accuracy of Bagging Tree:\", mean_accuracy)\n",
    "\n",
    "# Perform majority voting on the predictions of all trees\n",
    "predictions = []\n",
    "for sample in X:\n",
    "    tree_predictions = [tree.predict([sample])[0] for tree in trees]\n",
    "    prediction = max(set(tree_predictions), key=tree_predictions.count)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = sum([1 for i in range(len(predictions)) if predictions[i] == y[i]]) / len(predictions)\n",
    "print(\"Overall Accuracy:\", overall_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)When each node of the decision tree is split, 3 features are randomly selected in the way of\n",
    "non-replacement sampling, and task 2) is performed accordingly. (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of RandomForest: 0.8261130565282642\n"
     ]
    }
   ],
   "source": [
    "import random                                                                                  \n",
    "from DecisionTree import DecisionTree, load_data, compute_accuracy, k_fold_cross_validation                                                                                                                                               \n",
    "X, y = load_data('D:\\dts205tc\\data1.csv')                                                                                                   \n",
    "# Initialize a list to store the trees                                                         \n",
    "trees = []                                                                                     \n",
    "                                                                                               \n",
    "# Perform 100 bootstrapping                                                                    \n",
    "for i in range(100):                                                                           \n",
    "    # Create a bootstrap sample                                                                \n",
    "    sample_indices = [random.randint(0, len(X) - 1) for _ in range(len(X))]                    \n",
    "    X_sample = [X[j] for j in sample_indices]                                                  \n",
    "    y_sample = [y[j] for j in sample_indices]                                                  \n",
    "                                                                                               \n",
    "    # Initialize and fit a decision tree                                                       \n",
    "    tree = DecisionTree()                                                                      \n",
    "                                                                                               \n",
    "    # Randomly select 3 features without replacement                                           \n",
    "    feature_indices = random.sample(range(len(X[0])), 3)                                       \n",
    "    X_sample = [[x[i] for i in feature_indices] for x in X_sample]                             \n",
    "                                                                                               \n",
    "    tree.fit(X_sample, y_sample)                                                               \n",
    "                                                                                               \n",
    "    # Append the tree to the list                                                              \n",
    "    trees.append(tree)                                                                         \n",
    "                                                                                               \n",
    "                                                                                               \n",
    "def predict(X):                                                                                \n",
    "    # Make predictions for each tree                                                           \n",
    "    predictions = [tree.predict(X[:, feature_indices]) for tree in trees]                      \n",
    "                                                                                               \n",
    "    # Take the majority vote as the final prediction                                           \n",
    "    y_pred = []                                                                                \n",
    "    for i in range(len(X)):                                                                    \n",
    "        counts = {label: 0 for label in set(y)}                                                \n",
    "        for prediction in predictions:                                                         \n",
    "            counts[prediction[i]] += 1                                                         \n",
    "        y_pred.append(max(counts, key=counts.get))                                             \n",
    "    return y_pred                                                                              \n",
    "                                                                                               \n",
    "                                                                                                                                                  \n",
    "tree = DecisionTree()                                                                          \n",
    "mean_accuracy = k_fold_cross_validation(X,y,k=5, model=tree)                         \n",
    "print(f\"Mean accuracy of RandomForest: {mean_accuracy}\")                                       \n",
    "                                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.7426999999999999\n"
     ]
    }
   ],
   "source": [
    "# Single-layer Decision Tree\n",
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.feature_idx = None\n",
    "        self.threshold = None\n",
    "        self.label = None\n",
    "\n",
    "\n",
    "class DecisionTreeSingle:\n",
    "    def __init__(self, min_samples_split=2):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def calculate_entropy(self, labels):\n",
    "        _, counts = np.unique(labels, return_counts=True)\n",
    "        probabilities = counts / counts.sum()\n",
    "        entropy = sum(probabilities * -np.log2(probabilities))\n",
    "        return entropy\n",
    "\n",
    "    def calculate_info_gain(self, parent, left_child, right_child):\n",
    "        weighted_child_entropy = sum(\n",
    "            [len(child_labels) / len(parent) * self.calculate_entropy(child_labels) for child_labels in\n",
    "             [left_child, right_child]])\n",
    "        parent_entropy = self.calculate_entropy(parent)\n",
    "        info_gain = parent_entropy - weighted_child_entropy\n",
    "        return info_gain\n",
    "\n",
    "    def find_best_split(self, data, labels):\n",
    "        n_features = len(data[0])\n",
    "        best_gain = -1\n",
    "        best_feature_idx = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            feature_values = [data[i][feature_idx] for i in range(len(data))]\n",
    "            unique_values = set(feature_values)\n",
    "\n",
    "            for threshold in unique_values:\n",
    "                left_labels = [labels[i] for i in range(len(labels)) if data[i][feature_idx] <= threshold]\n",
    "                right_labels = [labels[i] for i in range(len(labels)) if data[i][feature_idx] > threshold]\n",
    "\n",
    "                if len(left_labels) == 0 or len(right_labels) == 0:\n",
    "                    continue\n",
    "\n",
    "                gain = self.calculate_info_gain(labels, left_labels, right_labels)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature_idx = feature_idx\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature_idx, best_threshold\n",
    "\n",
    "    def build_tree(self, data, labels):\n",
    "        n_samples = len(data)\n",
    "\n",
    "        # Base case: only 1 sample or all samples belong to the same class\n",
    "        if len(set(labels)) == 1 or n_samples < self.min_samples_split:\n",
    "            return Node(data, labels)\n",
    "\n",
    "        # Recursive case: find the best split and create sub-trees\n",
    "        best_feature_idx, best_threshold = self.find_best_split(data, labels)\n",
    "        left_idxs = [idx for idx in range(n_samples) if data[idx][best_feature_idx] <= best_threshold]\n",
    "        right_idxs = [idx for idx in range(n_samples) if data[idx][best_feature_idx] > best_threshold]\n",
    "\n",
    "        # Ensure both left and right child contain at least one sample\n",
    "        if len(left_idxs) > 0 and len(right_idxs) > 0:\n",
    "            left_data = [data[idx] for idx in left_idxs]\n",
    "            left_labels = [labels[idx] for idx in left_idxs]\n",
    "            right_data = [data[idx] for idx in right_idxs]\n",
    "            right_labels = [labels[idx] for idx in right_idxs]\n",
    "\n",
    "            node = Node(data, labels)\n",
    "            node.feature_idx = best_feature_idx\n",
    "            node.threshold = best_threshold\n",
    "            node.left = self.build_tree(left_data, left_labels)\n",
    "            node.right = self.build_tree(right_data, right_labels)\n",
    "\n",
    "            return node\n",
    "\n",
    "    def traverse_tree(self, x, node):\n",
    "        if node.label is not None:\n",
    "            return node.label\n",
    "        if x[node.feature_idx] <= node.threshold:\n",
    "            return self.traverse_tree\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self.build_tree(X, y)\n",
    "\n",
    "    def predict(self, data):\n",
    "        predictions = []\n",
    "        for sample in data:\n",
    "            node = self.root\n",
    "            while node.left and node.right:\n",
    "                if sample[node.feature_idx] <= node.threshold:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "            predictions.append(max(set(node.labels), key=node.labels.count))\n",
    "        return predictions\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    X = []  # features\n",
    "    y = []  # target labels\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # skip header row\n",
    "        for row in reader:\n",
    "            X.append([float(x) for x in row[:-1]])\n",
    "            y.append(int(row[-1]))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def k_fold_cross_validation(data, labels, k, model):\n",
    "    fold_size = len(data) // k\n",
    "    indices = list(range(len(data)))\n",
    "    random.shuffle(indices)\n",
    "    accuracies = []\n",
    "    for i in range(k):\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size\n",
    "        validation_indices = indices[start:end]\n",
    "        training_indices = indices[:start] + indices[end:]\n",
    "        training_data = [data[idx] for idx in training_indices]\n",
    "        training_labels = [labels[idx] for idx in training_indices]\n",
    "        validation_data = [data[idx] for idx in validation_indices]\n",
    "        validation_labels = [labels[idx] for idx in validation_indices]\n",
    "        model.fit(training_data, training_labels)\n",
    "        predictions = model.predict(validation_data)\n",
    "        accuracy = compute_accuracy(predictions, validation_labels)\n",
    "        accuracies.append(accuracy)\n",
    "    mean_accuracy = sum(accuracies) / k\n",
    "    return mean_accuracy\n",
    "\n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    assert len(predictions) == len(labels)\n",
    "    n_correct = sum([1 if p == l else 0 for p, l in zip(predictions, labels)])\n",
    "    accuracy = n_correct / len(predictions)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "data, labels = load_data('D:\\dts205tc\\data.csv')\n",
    "tree = DecisionTreeSingle(min_samples_split=2)\n",
    "mean_accuracy = k_fold_cross_validation(data, labels, k=5, model=tree)\n",
    "print(f\"Mean accuracy: {mean_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Based on task 2) single-layer decision tree, 3) decision tree, 4) bagging tree, 5) random forest,\n",
    "perform 5-fold CV, and compare their average prediction Accuracy on the validation set. (10 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Single-layer Decision Tree</th>\n",
       "      <td>0.742700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.821011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Tree</th>\n",
       "      <td>0.849725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.826113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Accuracy\n",
       "Single-layer Decision Tree  0.742700\n",
       "Decision Tree               0.821011\n",
       "Bagging Tree                0.849725\n",
       "Random Forest               0.826113"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "form={\"Accuracy\":[ 0.7426999999999999,0.8210105052526263,0.8497248624312157,0.8261130565282642]}\n",
    "df=pd.DataFrame(form,index=['Single-layer Decision Tree','Decision Tree','Bagging Tree','Random Forest'] )\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The current device information:\n",
    "processor : Intel(R) Core(TM) i7-10710U CPU @ 1.10GHz   1.61 GHz,Ram \n",
    "Ram: 16 GB\n",
    "So it is hard for the device to run the original dataset. I tried the original dataset with the DecisionTree model, which takes more than 20 minutes to get the result, so dataset was modified as data1.csv which contains 10000 rows of the original dataset.\n",
    "Although accuracy was optimized by randomly shuffling the data, it may still be affected by the modification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afc7c020546840ac4da3cfe251788a271998f1b80367e3a17b2eb7d8b954e3a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
